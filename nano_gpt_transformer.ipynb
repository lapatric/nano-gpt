{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f98521c5e30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task**\n",
    "\n",
    "We train a baseline language model on the Tiny Shakespeare corpus. In particular, we will use character-level tokenization, and predict the next character given the previous $k$ characters (conditional probability over characteers). To achieve this we will train the model to maximize the log-likelihood of the data. \n",
    "\n",
    "$$\n",
    "\\max_{\\theta} \\sum_{i=1}^{N} \\hat{p}(\\mathbf{x}_i) = \\max_{\\theta} \\sum_{i=1}^{N} \\prod_{j=1}^{k} \\hat{p}(x_j | x_{j-1},..., x_{j-k}; \\theta) = \\max_{\\theta} \\sum_{i=1}^{N} \\sum_{j=1}^{k} \\log \\hat{p}(x_j | x_{j-1},..., x_{j-k}; \\theta)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i$ is the $i$-th sequence in the dataset created from the corpus, $N$ is the number of sequences in the dataset and $\\theta$ are the parameters of the model. \n",
    "\n",
    "### Multiple context lengths $k$\n",
    "\n",
    "For large language models (LLMs) such as GPT-2, we in fact train over various context lengths $k$, e.g. we may set a maximum context length of $K$ and train the model on all sequences of length $K$ or less. The above formulation is therefore slightly simplified, but the idea is the same. This modification will also allow our model to learn how to generate text based on only a single character which is useful for generating text from scratch (beginning only with a \"space\" character).\n",
    "\n",
    "In the context of creating a dataset, we can think of passing the model a sequence of length $K$ but instead of only predicting the next character based on the entire sequence we will predict the next character for each subsequence of length $k \\leq K$. Therefore,  our target will consist of the next character for each subsequence of length $k$:\n",
    "\n",
    "$$\n",
    "[x_{1}, x_{2}, ..., x_{K}] \\mapsto [x_{2}, x_{3}, ..., x_{K+1}] \n",
    "$$ \n",
    "\n",
    "In particular, the following subsequence predictions are made:\n",
    "\n",
    "$$\n",
    "x_{1} \\mapsto x_{2} \\\\\n",
    "x_{1}, x_{2} \\mapsto x_{3} \\\\ \n",
    "x_{1}, x_{2}, x_{3} \\mapsto x_{4} \\\\\n",
    "... \\\\\n",
    "x_{1}, x_{2}, ..., x_{K} \\mapsto x_{K+1}\n",
    "$$\n",
    "\n",
    "When using a transformer (decoder) architecture, we can use the same input sequence for all subsequence predictions, but mask out the positions that are not relevant for the prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No. characteres: 1115393. Unique set: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "corpus = open('tiny_shakespeare.txt', 'r').read()\n",
    "# print(corpus[:100], '...', sep=\"\")\n",
    "\n",
    "# map each character to an integer and vice versa\n",
    "chars = sorted(set(corpus))\n",
    "idx2char = dict(enumerate(chars))\n",
    "char2idx = {v: k for (k, v) in idx2char.items()}\n",
    "print(f'\\nNo. characteres: {len(corpus)}. Unique set: {chars}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `get_batch(split)` method as opposed to a sliding window approach to precompile all possible batches. This is due to the heavy overlap between sequences and the amount of memory it would require ($K$ times more). Instead, we will use a random sampling approach to generate batches on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode corpus (char to int)\n",
    "enc_corpus = [char2idx[i] for i in corpus[:10000]]\n",
    "\n",
    "# split into train and validation sets\n",
    "split_idx = int(len(enc_corpus)*0.9)\n",
    "enc_train_corpus = enc_corpus[:split_idx]\n",
    "enc_val_corpus = enc_corpus[split_idx:]\n",
    "\n",
    "batch_idxs = {'train': [], 'val': []}\n",
    "def get_batch(split, batch_size=32, block_size=128):\n",
    "    enc_corpus = enc_train_corpus if split == 'train' else enc_val_corpus\n",
    "    idxs = torch.randint(len(enc_corpus) - block_size, (batch_size,))\n",
    "    batch_idxs[split] += idxs.tolist()\n",
    "    xb = torch.stack([torch.tensor(enc_corpus[i:i+block_size]) for i in idxs]) # stack along batch dimension (0)\n",
    "    yb = torch.stack([torch.tensor(enc_corpus[i+1:i+1+block_size]) for i in idxs])\n",
    "    return xb, yb\n",
    "\n",
    "# X, Y = get_batch('train', batch_size=3, block_size=10)\n",
    "# print(f\"X.shape: {X.shape} | X.dtype: {X.dtype}\")\n",
    "# print(f\"Y.shape: {Y.shape} | Y.dtype: {Y.dtype}\")\n",
    "# for i in range(3):\n",
    "#     print(f\"{X[i]} ---> {Y[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize sampled batch indices\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Sampled batch indices')\n",
    "plt.hist(batch_idxs['train'], bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GPT model**\n",
    "\n",
    "In this notebook we present a solution using the transformer architecture. As we are only interested in generating text without any context or prompt, we will only use the decoder part of the transformer. We begin by implementing the decoder stack without any modularity in order simplify the code and provide a clear overview of the model. We will then refactor the code to make it more modular and reusable.\n",
    "\n",
    "When devising such a model, it is important to increase complexity gradually in order to ensure that the model is able to learn and benefitting from the added complexity. Consequently, one would generally begin without any attention layers and a reduced context length. Then one may increase the context length and use a simplified communication scheme between the tokens (e.g. mean pooling). Finally, one may add attention layers and increase the number of layers. In the below code we skip this process and jump straight to the final model for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_s = 3     # batch size\n",
    "seq_l = 5       # sequence length\n",
    "vocab_s = 6     # vocab size\n",
    "emb_d = 8       # embedding dimension\n",
    "h = 2           # number of heads\n",
    "\n",
    "# basic outline of transformer: \n",
    "x_batch = torch.randint(0, vocab_s, (batch_s, seq_l)) # (batch_size, seq_len)\n",
    "\n",
    "# embedding layer\n",
    "W_emb = torch.randn(vocab_s, emb_d) # (vocab_size, embedding_dim)\n",
    "x_emb = W_emb[x_batch] # (batch_size, seq_len, embedding_dim)\n",
    "# batch_size, seq_len and embedding_dim are often referred to as B, T and C in the literature,\n",
    "# which stands for batch dimension, temporal dimension and channel dimension, respectively.\n",
    "\n",
    "# positional encoding\n",
    "W_pos = torch.randn(seq_l, emb_d) # (seq_len, embedding_dim)\n",
    "pos_idxs = torch.arange(seq_l) # (seq_len,)\n",
    "x_pos = W_pos[pos_idxs] # (seq_len, embedding_dim)\n",
    "x_emb = x_emb + x_pos # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "# masked multi-head self-attention \n",
    "W_kqv = torch.randn(emb_d, 3*emb_d) # (embedding_dim, 3*embedding_dim)\n",
    "k, q, v = (x_emb @ W_kqv).split(emb_d, dim=-1) # (batch_size, seq_len, embedding_dim) (3x)\n",
    "# We split the embedding dimension into h equal parts (one for each attention head).\n",
    "# Subsequently, we transpose the 1st and 2nd dimension to have batch_s * h batches of \n",
    "# seq_l x (emb_d // 2) matrices over which attention is performed.\n",
    "# When devising such efficient implementations, it is important to test the correctness on small examples.\n",
    "k = k.view(batch_s, seq_l, h, emb_d // h).transpose(1, 2)\n",
    "q = q.view(batch_s, seq_l, h, emb_d // h).transpose(1, 2)\n",
    "v = v.view(batch_s, seq_l, h, emb_d // h).transpose(1, 2)\n",
    "\n",
    "# We now have a set of `seq_l` queries, keys and values. Each query (projected to from a token/character, embedding) is used to \n",
    "# generate a new embedding for said token/character based on its (≤seq_l) predecessors. To ensure that it only consists of \n",
    "# its predecessors, we mask out the affinities/weights to all tokens that come after it. This is done prior to applying softmax, \n",
    "# by setting those weights to `-inf`. After softmax, these normalized weights become `exp(-inf)/sum(...)` which is 0. This masking \n",
    "# process to ensure that the weights are only based on the tokens that come before the current token is called causal masking and \n",
    "# is what differentiates a decoder transformer from an encoder transformer. \n",
    "tril_mask = torch.ones((seq_l, seq_l)).tril()\n",
    "wei = q @ k.transpose(-2, -1) / ((emb_d // 2) ** 1/2) # dot product amplifies variance ~> scale back to unit variance \n",
    "wei = wei.masked_fill(tril_mask == 0, -torch.inf)\n",
    "wei = F.softmax(q @ k.transpose(-2, -1), -1) # (batch_size, h, seq_len, seq_len)\n",
    "x_emb_att = wei @ v # (batch_size, h, seq_len, embedding_dim // h)\n",
    "x_emb_att = x_emb_att.transpose(1, 2).contiguous().view(batch_s, seq_l, emb_d) # (batch_size, seq_len, embedding_dim)\n",
    "W_o = torch.randn(emb_d, emb_d) # (embedding_dim, embedding_dim)\n",
    "x_emb_att = x_emb_att @ W_o # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "# add & norm\n",
    "x_emb = x_emb + x_emb_att\n",
    "x_emb = F.layer_norm(x_emb, (emb_d,)) # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "# cross-attention module\n",
    "# As we do not require any cross-attention over a prompt or context, we skip this step.\n",
    "\n",
    "# feed-forward module\n",
    "d_ff = 4 * emb_d\n",
    "W_ff1 = torch.randn((emb_d, d_ff))\n",
    "W_ff2 = torch.randn((d_ff, emb_d))\n",
    "x_ff = F.relu(x_emb @ W_ff1) @ W_ff2 # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "# add & norm\n",
    "x_emb = x_emb + x_ff\n",
    "x_emb = F.layer_norm(x_emb, (emb_d,)) # (batch_size, seq_len, embedding_dim)\n",
    "# In contrast to batch normalization, layer normalization normalizes over the embedding or channel dimension.\n",
    "# This means that there is also no distinction between training and evaluation time, as the normalization\n",
    "# is not based on the batch statistics.\n",
    "\n",
    "# output layer\n",
    "W_out = torch.randn((emb_d, vocab_s)) \n",
    "x_out = x_emb @ W_out # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "# softmax\n",
    "logits = F.softmax(x_emb, dim=-1) # (batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules of the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.W_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.W_pos = nn.Parameter(torch.randn(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_emb = self.W_emb(x) + self.W_pos\n",
    "        return x_emb\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.W_kqv = nn.Linear(emb_dim, 3*emb_dim)\n",
    "        self.W_out = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       B, T, C = x.shape\n",
    "       k, q, v = self.W_kqv(x).split(C, dim=-1) # (batch_size, seq_len, embedding_dim) (3x)\n",
    "       k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2) \n",
    "       q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
    "       v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
    "\n",
    "       wei = q @ k.transpose(-2, -1) / ((C // self.num_heads) ** 1/2)\n",
    "       tril_mask = torch.ones((T, T)).tril()\n",
    "       wei = wei.masked_fill(tril_mask == 0, -torch.inf)\n",
    "       wei = F.softmax(wei, -1) # can add dropout after softmax\n",
    "       x_att = (wei @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "       return self.dropout(self.W_out(x_att))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4*emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*emb_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.ff = FeedForward(emb_dim)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_att = self.att(x)\n",
    "        x_att = self.norm1(x + x_att)\n",
    "        x_ff = self.ff(x_att)\n",
    "        x_ff = self.norm2(x_att + x_ff)\n",
    "        return x_ff\n",
    "       \n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(vocab_size, emb_dim)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(emb_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.W_out = nn.Linear(emb_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_emb = self.emb(x)\n",
    "        for block in self.blocks:\n",
    "            x_emb = block(x_emb)\n",
    "        x_emb = self.W_out(x_emb)\n",
    "        return x_emb # careful not to apply softmax here as nn.CrossEntropy already does that for us!\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, enc_text, max_new_tokens):\n",
    "        self.eval()\n",
    "        # keep generated text in tensor instead of a list and converting to tensor\n",
    "        # in each loop. That is not efficient + alot of GPU I/O if using CUDA.\n",
    "        for _ in range(max_new_tokens):\n",
    "            probs = F.softmax(self(enc_text[:, -block_size:]), dim=-1) # (B=1, ≤block_size, vocab_size) \n",
    "            idx_next = torch.multinomial(probs[0, -1, :], num_samples=1) \n",
    "            enc_text = torch.cat((enc_text, idx_next.view(1, 1)), dim=1) # (B=1, ≤block_size)\n",
    "            \n",
    "        return enc_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 3219521\n"
     ]
    }
   ],
   "source": [
    "# reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 20\n",
    "vocab_size = len(chars)\n",
    "emb_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 1\n",
    "lr = 1e-3\n",
    "max_iters = 1000\n",
    "eval_interval = max_iters // 50\n",
    "eval_iters = 20\n",
    "\n",
    "# instantiate model\n",
    "model = TransformerLM(vocab_size, emb_dim, num_heads, num_layers)\n",
    "print(f\"Number of model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# model evaluation loop\n",
    "# provides a more accurate estimate than the batch-wise loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        total_loss = 0\n",
    "        for _ in range(eval_iters):\n",
    "            xb, yb = get_batch(split, batch_size, block_size)\n",
    "            logits = model(xb).view(-1, vocab_size)\n",
    "            total_loss += loss_fn(logits, yb.view(-1)).item()\n",
    "        out[split] = total_loss / eval_iters\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train_loss=3.70230, val_loss=3.83775\n",
      "step 20: train_loss=2.50761, val_loss=2.67941\n",
      "step 40: train_loss=2.33172, val_loss=2.51880\n",
      "step 60: train_loss=2.25502, val_loss=2.46543\n",
      "step 80: train_loss=2.24508, val_loss=2.49685\n",
      "step 100: train_loss=2.14839, val_loss=2.41994\n",
      "step 120: train_loss=2.14030, val_loss=2.47317\n",
      "step 140: train_loss=2.09904, val_loss=2.40906\n",
      "step 160: train_loss=2.07985, val_loss=2.45160\n",
      "step 180: train_loss=2.09857, val_loss=2.46383\n",
      "step 200: train_loss=2.03247, val_loss=2.44388\n",
      "step 220: train_loss=1.99383, val_loss=2.48788\n",
      "step 240: train_loss=1.97914, val_loss=2.47375\n",
      "step 260: train_loss=1.92629, val_loss=2.45564\n",
      "step 280: train_loss=1.89051, val_loss=2.47387\n",
      "step 300: train_loss=1.91572, val_loss=2.50348\n",
      "step 320: train_loss=1.89154, val_loss=2.49757\n",
      "step 340: train_loss=1.86124, val_loss=2.52629\n",
      "step 360: train_loss=1.84460, val_loss=2.51683\n",
      "step 380: train_loss=1.80617, val_loss=2.53839\n",
      "step 400: train_loss=1.77250, val_loss=2.59160\n",
      "step 420: train_loss=1.75041, val_loss=2.60959\n",
      "step 440: train_loss=1.73118, val_loss=2.60800\n",
      "step 460: train_loss=1.70444, val_loss=2.59766\n",
      "step 480: train_loss=1.70103, val_loss=2.63205\n",
      "step 500: train_loss=1.68406, val_loss=2.64807\n",
      "step 520: train_loss=1.62266, val_loss=2.71432\n",
      "step 540: train_loss=1.60553, val_loss=2.72661\n",
      "step 560: train_loss=1.59689, val_loss=2.76344\n",
      "step 580: train_loss=1.59636, val_loss=2.72757\n",
      "step 600: train_loss=1.53394, val_loss=2.82454\n",
      "step 620: train_loss=1.54170, val_loss=2.75115\n",
      "step 640: train_loss=1.52856, val_loss=2.85419\n",
      "step 660: train_loss=1.55004, val_loss=2.88860\n",
      "step 680: train_loss=1.46230, val_loss=2.84046\n",
      "step 700: train_loss=1.45380, val_loss=2.91514\n",
      "step 720: train_loss=1.46844, val_loss=2.90183\n",
      "step 740: train_loss=1.41181, val_loss=2.91300\n",
      "step 760: train_loss=1.40087, val_loss=3.02046\n",
      "step 780: train_loss=1.36017, val_loss=2.99900\n",
      "step 800: train_loss=1.36110, val_loss=3.03980\n",
      "step 820: train_loss=1.31214, val_loss=2.99541\n",
      "step 840: train_loss=1.31394, val_loss=2.98377\n",
      "step 860: train_loss=1.27635, val_loss=3.00833\n",
      "step 880: train_loss=1.26103, val_loss=3.07972\n",
      "step 900: train_loss=1.24287, val_loss=3.06375\n",
      "step 920: train_loss=1.26644, val_loss=3.09841\n",
      "step 940: train_loss=1.22875, val_loss=3.13115\n",
      "step 960: train_loss=1.21989, val_loss=3.19682\n",
      "step 980: train_loss=1.18208, val_loss=3.22152\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for i in range(max_iters):\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "\n",
    "    # forward pass & update\n",
    "    logits = model(xb).view(-1, vocab_size)\n",
    "    loss = loss_fn(logits, yb.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # every so often evaluate loss on train and validation sets\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step {i}: train_loss={losses[\"train\"]:.5f}, val_loss={losses[\"val\"]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let ust ret kinobrie ilang thive g shbut s thung s: athalll n t t wowe pl; rhaks, heak whofoneYe ses! What ,\n",
      "\n",
      "They, kivery alang\n",
      "d t the!\n",
      "Whe ne obaullllll and cacanstion\n",
      "cte imblitef uryisedan\n",
      "Rome you te te\n",
      "Theat stalis dit t atheirueir nup, andestor\n",
      "Mak,\n",
      "US:\n",
      "You r Is, 't, man st?\n",
      "\n",
      "\n",
      "\n",
      "Firste Cititizen:\n",
      "Well, I what sk. thand,\n",
      "\n",
      "First Cizen:\n",
      "He'Thast, as t t t wiserve will; when atus d 'ould lire he againcoding ance aing ctnge ty te staive s thbeand ririntoug\n",
      "I atnt t s sthalllheitop nthis\n",
      "I ore t far\n",
      "A all muswndosl\n",
      "Yos MAnturstur re ar o,\n",
      "Marcius!\n",
      "\n",
      "\n",
      "MARCIUS:\n",
      "Hanke -irom t t thonglincrainst as, e t mane cite\n",
      "Yo me man mman mu al e inse we in\n",
      "Ifors whads  ght t norvithe thigo's todsthe at rmbod,\n",
      "And t s nd, ing thimungh frt t ysthe ris wicouland frse h rse ors thamisende tsto t baitshing th thththth?\n",
      "B\n",
      "Whirvereryoury t irs f uryou blu wand prs cinecindalaceste, fio mion\n",
      "And t I ndlsting theast t they-cromminche hencancang we al a a st t s to!\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "The contoul mnitry th,\n",
      "The im wa\n"
     ]
    }
   ],
   "source": [
    "num_gen_tokens = 1000\n",
    "start = torch.zeros((1, 1), dtype=torch.long)\n",
    "enc_text = model.generate(start, max_new_tokens=num_gen_tokens)\n",
    "print(''.join([idx2char[i] for i in enc_text.flatten().tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
