{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task**\n",
    "\n",
    "We train a baseline language model on the Tiny Shakespeare corpus. In particular, we will use character-level tokenization, and predict the next character given the previous $k$ characters (conditional probability over characteers). To achieve this we will train the model to maximize the log-likelihood of the data. \n",
    "\n",
    "$$\n",
    "\\max_{\\theta} \\sum_{i=1}^{N} \\hat{p}(\\mathbf{x}_i) = \\max_{\\theta} \\sum_{i=1}^{N} \\prod_{j=1}^{k} \\hat{p}(x_j | x_{j-1},..., x_{j-k}; \\theta) = \\max_{\\theta} \\sum_{i=1}^{N} \\sum_{j=1}^{k} \\log \\hat{p}(x_j | x_{j-1},..., x_{j-k}; \\theta)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i$ is the $i$-th sequence in the dataset created from the corpus, $N$ is the number of sequences in the dataset and $\\theta$ are the parameters of the model. \n",
    "\n",
    "### Multiple context lengths $k$\n",
    "\n",
    "For large language models (LLMs) such as GPT-2, we in fact train over various context lengths $k$, e.g. we may set a maximum context length of $K$ and train the model on all sequences of length $K$ or less. The above formulation is therefore slightly simplified, but the idea is the same. This modification will also allow our model to learn how to generate text based on only a single character which is useful for generating text from scratch (beginning only with a \"space\" character).\n",
    "\n",
    "In the context of creating a dataset, we can think of passing the model a sequence of length $K$ but instead of only predicting the next character based on the entire sequence we will predict the next character for each subsequence of length $k \\leq K$. Therefore,  our target will consist of the next character for each subsequence of length $k$:\n",
    "\n",
    "$$\n",
    "[x_{1}, x_{2}, ..., x_{K}] \\mapsto [x_{2}, x_{3}, ..., x_{K+1}] \n",
    "$$ \n",
    "\n",
    "In particular, the following subsequence predictions are made:\n",
    "\n",
    "$$\n",
    "x_{1} \\mapsto x_{2} \\\\\n",
    "x_{1}, x_{2} \\mapsto x_{3} \\\\ \n",
    "x_{1}, x_{2}, x_{3} \\mapsto x_{4} \\\\\n",
    "... \\\\\n",
    "x_{1}, x_{2}, ..., x_{K} \\mapsto x_{K+1}\n",
    "$$\n",
    "\n",
    "When using a transformer (decoder) architecture, we can use the same input sequence for all subsequence predictions, but mask out the positions that are not relevant for the prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You...\n",
      "\n",
      "No. characteres: 1115393. Unique set: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "corpus = open('tiny_shakespeare.txt', 'r').read()\n",
    "print(corpus[:100], '...', sep=\"\")\n",
    "\n",
    "# map each character to an integer and vice versa\n",
    "chars = sorted(set(corpus))\n",
    "idx2char = dict(enumerate(chars))\n",
    "char2idx = {v: k for (k, v) in idx2char.items()}\n",
    "print(f'\\nNo. characteres: {len(corpus)}. Unique set: {chars}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `get_batch(split)` method as opposed to a sliding window approach to precompile all possible batches. This is due to the heavy overlap between sequences and the amount of memory it would require ($K$ times more). Instead, we will use a random sampling approach to generate batches on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form dataset from corpus\n",
    "block_size = 20 # context size for next character prediction\n",
    "batch_size = 32 # batch size for training\n",
    "\n",
    "# encode corpus (char to int)\n",
    "enc_corpus = [char2idx[i] for i in corpus[:1000]]\n",
    "\n",
    "# split into train and validation sets\n",
    "split_idx = int(len(enc_corpus)*0.9)\n",
    "enc_train_corpus = enc_corpus[:split_idx]\n",
    "enc_val_corpus = enc_corpus[split_idx:]\n",
    "\n",
    "def get_batch(split):\n",
    "    enc_corpus = enc_train_corpus if split == 'train' else enc_val_corpus\n",
    "    idxs = torch.randint(len(enc_corpus) - block_size, (batch_size,))\n",
    "    X_batch = torch.stack([torch.tensor(enc_corpus[i:i+block_size]) for i in idxs]) # stack along batch dimension (0)\n",
    "    Y_batch = torch.stack([torch.tensor(enc_corpus[i+1:i+1+block_size]) for i in idxs])\n",
    "    return X_batch, Y_batch\n",
    "\n",
    "# X, Y = get_batch('train')\n",
    "# print(f\"X.shape: {X.shape} | X.dtype: {X.dtype}\")\n",
    "# print(f\"Y.shape: {Y.shape} | Y.dtype: {Y.dtype}\")\n",
    "# for i in range(3):\n",
    "#     print(f\"{X[i]} ---> {Y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transformer model**\n",
    "\n",
    "In this notebook we present a solution using the transformer architecture. As we are only interested in generating text without any context or prompt, we will only use the decoder part of the transformer. We begin by implementing the decoder stack without any modularity in order simplify the code and provide a clear overview of the model. We will then refactor the code to make it more modular and reusable.\n",
    "\n",
    "When devising such a model, it is important to increase complexity gradually in order to ensure that the model is able to learn and benefitting from the added complexity. Consequently, one would generally begin without any attention layers and a reduced context length. Then one may increase the context length and use a simplified communication scheme between the tokens (e.g. mean pooling). Finally, one may add attention layers and increase the number of layers. In the below code we skip this process and jump straight to the final model for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_s = 3     # batch size\n",
    "seq_l = 5       # sequence length\n",
    "vocab_s = 6     # vocab size\n",
    "emb_d = 8       # embedding dimension\n",
    "h = 2           # number of heads\n",
    "\n",
    "# basic outline of transformer: \n",
    "x_batch = torch.randint(0, vocab_s, (batch_s, seq_l)) # (batch_size, seq_len)\n",
    "\n",
    "# embedding layer\n",
    "W_emb = torch.randn(vocab_s, emb_d) # (vocab_size, embedding_dim)\n",
    "x_emb = W_emb[x_batch] # (batch_size, seq_len, embedding_dim)\n",
    "# batch_size, seq_len and embedding_dim are often referred to as B, T and C in the literature,\n",
    "# which stands for batch dimension, temporal dimension and channel dimension, respectively.\n",
    "\n",
    "# positional encoding\n",
    "W_pos = torch.randn(seq_l, emb_d) # (seq_len, embedding_dim)\n",
    "pos_idxs = torch.arange(seq_l) # (seq_len,)\n",
    "x_pos = W_pos[pos_idxs] # (seq_len, embedding_dim)\n",
    "x_emb = x_emb + x_pos # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "# masked multi-head self-attention \n",
    "W_kqv = torch.randn(emb_d, 3*emb_d) # (embedding_dim, 3*embedding_dim)\n",
    "k, q, v = (x_emb @ W_kqv).split(emb_d, dim=-1) # (batch_size, seq_len, embedding_dim) (3x)\n",
    "# We split the embedding dimension into h equal parts (one for each attention head).\n",
    "# Subsequently, we transpose the 1st and 2nd dimension to have batch_s * h batches of \n",
    "# seq_l x (emb_d // 2) matrices over which attention is performed.\n",
    "# When devising such efficient implementations, it is important to test the correctness on small examples.\n",
    "k = k.view(batch_s, seq_l, h, emb_d // h).transpose(1, 2)\n",
    "q = q.view(batch_s, seq_l, h, emb_d // h).transpose(1, 2)\n",
    "v = v.view(batch_s, seq_l, h, emb_d // h).transpose(1, 2)\n",
    "\n",
    "# We now have a set of `seq_l` queries, keys and values. Each query (projected to from a token/character, embedding) is used to \n",
    "# generate a new embedding for said token/character based on its (â‰¤seq_l) predecessors. To ensure that it only consists of \n",
    "# its predecessors, we mask out the affinities/weights to all tokens that come after it. This is done prior to applying softmax, \n",
    "# by setting those weights to `-inf`. After softmax, these normalized weights become `exp(-inf)/sum(...)` which is 0. This masking \n",
    "# process to ensure that the weights are only based on the tokens that come before the current token is called causal masking and \n",
    "# is what differentiates a decoder transformer from an encoder transformer. \n",
    "tril_mask = torch.ones((seq_l, seq_l)).tril()\n",
    "wei = q @ k.transpose(-2, -1) / ((emb_d // 2) ** 1/2) # dot product amplifies variance ~> scale back to unit variance \n",
    "wei = wei.masked_fill(tril_mask == 0, -torch.inf)\n",
    "wei = F.softmax(q @ k.transpose(-2, -1), -1) # (batch_size, h, seq_len, seq_len)\n",
    "x_emb_att = wei @ v # (batch_size, h, seq_len, embedding_dim // h)\n",
    "x_emb_att = x_emb_att.transpose(1, 2).contiguous().view(batch_s, seq_l, emb_d) # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "# add & norm\n",
    "x_emb = x_emb + x_emb_att\n",
    "x_emb = F.layer_norm(x_emb, (emb_d,)) # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "# cross-attention module\n",
    "# As we do not require any cross-attention over a prompt or context, we skip this step.\n",
    "\n",
    "# feed-forward module\n",
    "d_ff = 4 * emb_d\n",
    "W_ff1 = torch.randn((emb_d, d_ff))\n",
    "W_ff2 = torch.randn((d_ff, emb_d))\n",
    "x_ff = F.relu(x_emb @ W_ff1) @ W_ff2 # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "# add & norm\n",
    "x_emb = x_emb + x_ff\n",
    "x_emb = F.layer_norm(x_emb, (emb_d,)) # (batch_size, seq_len, embedding_dim)\n",
    "# In contrast to batch normalization, layer normalization normalizes over the embedding or channel dimension.\n",
    "# This means that there is also no distinction between training and evaluation time, as the normalization\n",
    "# is not based on the batch statistics.\n",
    "\n",
    "# output layer\n",
    "W_out = torch.randn((emb_d, vocab_s)) \n",
    "x_out = x_emb @ W_out # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "# softmax\n",
    "logits = F.softmax(x_emb, dim=-1) # (batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
